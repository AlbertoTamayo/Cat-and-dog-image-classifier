# Cat-and-dog-image-classifier
#IMPORTANT: Since this is a project from FreeCodeCamp's Machine Learning with Python course, some of the cells of this interactive notebook are already given by FreeCodeCamp in the default notebook. Only some cells were coded by me in order to complete the challenge based on the instructions given in the course material. The following cells were coded by me: (#3, #5, #7, and #8) as well the other cells that don't have comments at the top indicating the cell number. The first 2 cells, cell #4, #6, #9 and #11 were written by FCC in the base notebook in order to be later worked on and developed by those taking the course.

The script first imports all of the necessarry libraries for the code to run (additionally, it runs a special command to import tensorflow version 2.x which is only necessarry if run inside a Google Colab notebook).
The following cell extracts the training, validation and test data from a zip file from FCC's project data url and generates the necessarry paths to the various different splits of the data (training, validation and testing). It also defines the default batch_size, number of epochs for training the model as well as the dimensions to be used for the dataset images (pixel height x pixel width).

Cell #3 creates ImageDataGenerator objects from the keras.preprocessing module for all the dataset splits and rescales the images from values between 0 and 255 to between 0 and 1. The data generator objects are then created by extracting the data from the dataset paths using flow_from_directory() and defining the target  directory path, the target dimensions from the images, class mode (which is binary since the classifier is used on 2 classes) and the batch size for when the data is fed into the model for training. It's important to point out that the test data folder in this case doesn't have any subdirectories for the individual classes (which means unalabeled test data) and the "directory" argument in flow_from_directory must have subdirectories inside in order to function properly and separate the data, so that's why the root directory of the dataset zip folder is used as the base since it has subdirectories inside, and we assign ['test'] to the 'classes' argument in the flow_from_directory method to get around this situation. Additionally, class_mode is set to None because it is a testing dataset that doesn't have labels where predictions will be made and the 'shuffle' argument should be set to false for the test_data_gen in order to avoid shuffling the order of images to be fed during the prediction phase, ensuring reproducibility with an consistent test dataset.

Cell #4 plots 5 random training images from the train_data_gen created previously in order to verify correct creation the the training data generator.

Cell #5 creates a new version of the training data generator with some additional random image transformations in order to add more samples to the training data set since it has a rather small amount of images.

Cell #6 takes one random image from the new training generator and creates 5 different variations based on the transformation made previously.

Cell #7 creates the Keras.Sequential model using a stack of Conv2D layers with 'relu' activation functions and MaxPooling2D layers. Convultional layers in a neural network can do a better job than dense layers at classifying images since convolutional layers extract local patterns from images rather than global patterns which leads to a much greater flexibility when classifying images. For example, a convultional neural network can extract individual eyes or ears from a cat or dog without having to look at the image as a whole. The first layer should define the input size of the image, which after the transformations are 150 x 150 x 3 images (only for the first layer or input layer). Afterward, a dense layer is added in order to be able to actually classify the images, but before that, a new flattened layer must be created beforehand. After this other dense layer, a final dense layer with 1 neuron is created with a sigmoid activation function to squeeze the result between 0 and 1 to perform a binary classification.

The next cell compiles the model with a binary crossentropy loss function, a root-mean-squared propagation optimizer and only measures the model's accuracy. 

Cell #8 fits the model's parameters with the training and validation sets, the number of epochs and the steps per epoch to be taken while training. It's also important to point out that the steps for epoch should allow for the whole datasets to feed their data based on batch size and the total size of the training and validation data sizes.

Cell #9 displays 2 different plots of the evolution of the training and validation data's accuracy and loss evolving with each epoch of training.

The following 2 cells predicts the classes of the test data and calls the PlotImages() function from cell #4 to display the confidence values of all 50 samples of the test data based on the model

Cell #11 is the final check to see if the model accurately predicts at least 63% of the test images correctly as displayed in the challenge by FCC. I'd also like to point out that it took several tries to create a model that could achieve a level of this accuracy, and the main changes to try and optimize the model were in cells #5 (changing some image data transformations), #7 with the type of activation functions for each layer in the model and  #8 with the optimizer function for the back propagation (mainly switching from the Adam optimizer to RMSprop). With these settings, the final model so far leads to 72% accuracy on the test data fed to the model, though with more tuning, the model would most likely be even more accurate.
